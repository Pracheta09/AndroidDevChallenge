# -*- coding: utf-8 -*-
"""Word2Vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B9Q9qWWZ-HoabpPC4vhjffE29O5HisbI

Source : https://medium.com/@khulasaandh/word-embeddings-fun-with-word2vec-and-game-of-thrones-ea4c24fcf1b8
"""




"""Data Source : GOT books in text format

Link : https://www.kaggle.com/khulasasndh/game-of-thrones-books/downloads/game-of-thrones-books.zip/1
"""

''' Pre processing :

gensim.utils.simpl_preprocess-
 Convert a document into a list of tokens.
 This lowercases, tokenizes, de-accents (optional). – the output are final tokens = unicode strings, that won’t be processed any further.
'''
import os
from gensim.models import Word2Vec
from gensim.utils import simple_preprocess
from nltk.tokenize import sent_tokenize

#Below class is an iterator since it has the __iter__ method (in built iterators are list,tuple,etc)
#The __iter__ method is a generator 
#Generator-Object : Generator functions return a generator object. Generator objects are used either by calling the next method on the generator object or using the generator object in a “for in” loop

class MySentences(object):
  def __init__(self,dirname):
    self.dirname = dirname
    self.sentence_count = 0
    
  def __iter__(self):
    for fname in os.listdir(self.dirname):
      with open(os.path.join(self.dirname,fname) ) as f_input:
        corpus = f_input.read()
      raw_sentences = sent_tokenize(corpus)
      for sentence in raw_sentences:
        if len(sentence) > 0:
          self.sentence_count += 1
          yield simple_preprocess(sentence)
            
           
            
sentences = MySentences("F:/UBS/Artificial Intelligence/NLP/Got_dataset")
for i in sentences:
  print(i)

'''
Paper explaining Woed2Vec algo- CBoW and Skip gram- 
https://arxiv.org/pdf/1301.3781.pdf

Simple terms explaining how word is mapped to vec in word2vec-
https://medium.com/analytics-vidhya/neural-networks-for-word-embeddings-4b49e0e9c955


Word2Vec Tool-
https://code.google.com/archive/p/word2vec/

'''


#model = Word2Vec(sg=1,size=300,window=10,min_count=3,workers=4)
'''
sg Training algorithm: 1 for skip-gram; otherwise CBOW.
size Dimensionality of the word vectors. This parameter for altering the size of the NN layers. Bigger size values require more training data, but can lead to better (more accurate) models.
window The size of the window determines how many words before and after a given word would be included as context words of the given word.
min_count Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage, so its best to get rid of those.
workers Use these many worker threads to train the model.

For more detailed : Gensim API-
https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.html#gensim.models.Word2Vec

'''
'''
model.build_vocab(sentences)
model.train(sentences=sentences,total_examples=model.corpus_count,epochs=model.epochs)
model.save('GOT_vectors.w2v')
'''

